{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-15): 16 x TransformerBlock(\n",
       "      (ln1): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformer_lens.utils as utils\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "# from mech_interp_othello_utils import OthelloBoardState\n",
    "import einops\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from fancy_einsum import einsum\n",
    "import chess\n",
    "import numpy as np\n",
    "import csv\n",
    "import chess_utils\n",
    "from dataclasses import dataclass\n",
    "\n",
    "device = \"cuda\"\n",
    "# device = \"cpu\"\n",
    "device = \"mps\"\n",
    "\n",
    "n_layers = 16\n",
    "n_heads = 8\n",
    "MODEL_DIR = \"models/\"\n",
    "DATA_DIR = \"data/\"\n",
    "cfg = HookedTransformerConfig(\n",
    "    n_layers = n_layers,\n",
    "    d_model = 512,\n",
    "    d_head = 64,\n",
    "    n_heads = n_heads,\n",
    "    d_mlp = 2048,\n",
    "    d_vocab = 32,\n",
    "    n_ctx = 1023,\n",
    "    act_fn=\"gelu\",\n",
    "    normalization_type=\"LNPre\"\n",
    ")\n",
    "model = HookedTransformer(cfg)\n",
    "model.load_state_dict(torch.load(f'{MODEL_DIR}tf_lens_16.pth'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 12\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    min_val: int\n",
    "    max_val: int\n",
    "    custom_function: callable\n",
    "    linear_probe_name: str\n",
    "\n",
    "piece_config = Config(\n",
    "    min_val = -6,\n",
    "    max_val = 6,\n",
    "    custom_function = chess_utils.board_to_piece_state,\n",
    "    linear_probe_name = f\"{MODEL_DIR}chess_piece_probe_layer_{layer}.pth\",\n",
    ")\n",
    "\n",
    "color_config = Config(\n",
    "    min_val = -1,\n",
    "    max_val = 1,\n",
    "    custom_function=chess_utils.board_to_piece_color_state,\n",
    "    linear_probe_name=f\"{MODEL_DIR}chess_color_probe_layer_{layer}.pth\",\n",
    ")\n",
    "\n",
    "random_config = Config(\n",
    "    min_val = -1,\n",
    "    max_val = 1,\n",
    "    custom_function=chess_utils.board_to_random_state,\n",
    "    linear_probe_name=f\"{MODEL_DIR}chess_random_probe_layer_{layer}.pth\",\n",
    ")\n",
    "\n",
    "config = piece_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32965, 680])\n",
      "torch.Size([32965, 61])\n",
      "32965 680\n"
     ]
    }
   ],
   "source": [
    "board_seqs_int = torch.tensor(np.load(f\"{DATA_DIR}train_board_seqs_int.npy\")).long()\n",
    "print(board_seqs_int.shape)\n",
    "dots_indices = torch.tensor(np.load(f\"{DATA_DIR}train_dots_indices.npy\")).long()\n",
    "# state_stack = torch.tensor(np.load(\"state_stacks_5k.npy\")).long()\n",
    "print(dots_indices.shape)\n",
    "# print(state_stack.shape)\n",
    "\n",
    "board_seqs_string = []\n",
    "\n",
    "with open(f\"{DATA_DIR}train_board_seqs_string.csv\", newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        board_seqs_string.append(row[0])\n",
    "print(len(board_seqs_string), len(board_seqs_string[0]))\n",
    "# print(board_seqs_string[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(680, 8, 8)\n",
      "torch.Size([680, 8, 8])\n",
      "torch.Size([1, 50, 680, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# print(board_seqs_string[0])\n",
    "# custom_function = chess_utils.board_to_piece_state\n",
    "custom_function = config.custom_function\n",
    "print(chess_utils.create_state_stack(board_seqs_string[0], custom_function).shape)\n",
    "\n",
    "state_stack = torch.tensor(chess_utils.create_state_stack(board_seqs_string[0], custom_function)).long()\n",
    "print(state_stack.shape)\n",
    "\n",
    "state_stacks = chess_utils.create_state_stacks(board_seqs_string[:50], custom_function)\n",
    "print(state_stacks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 680, 8, 8, 13])\n",
      "tensor([[[[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]],\n",
      "\n",
      "         [[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]], device='mps:0')\n",
      "tensor([[[ 0, -3, -1],\n",
      "         [-2, -1,  0],\n",
      "         [ 0,  0,  0],\n",
      "         [ 0,  0, -6]]])\n"
     ]
    }
   ],
   "source": [
    "layer = 12\n",
    "batch_size = 1\n",
    "lr = 2e-4\n",
    "wd = 0.01\n",
    "pos_start = 0\n",
    "# pos_end = model.cfg.n_ctx - 5\n",
    "input_length = 680\n",
    "pos_end = input_length - 0\n",
    "length = pos_end - pos_start\n",
    "one_hot_range = config.max_val - config.min_val + 1\n",
    "rows = 8\n",
    "cols = 8\n",
    "num_epochs = 1\n",
    "num_games = 100\n",
    "x = 0\n",
    "y = 2\n",
    "# The first mode is blank or not, the second mode is next or prev GIVEN that it is not blank\n",
    "modes = 1\n",
    "alternating = torch.tensor([1 if i%2 == 0 else -1 for i in range(length)], device=device)\n",
    "\n",
    "\n",
    "state_stack_one_hot = chess_utils.state_stack_to_one_hot(modes, rows, cols, config.min_val, config.max_val, device, state_stacks)\n",
    "print(state_stack_one_hot.shape)\n",
    "print((state_stack_one_hot[:, 1, 170, 4:9, 2:5]))\n",
    "print((state_stacks[:, 1, 170, 4:9, 2:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 8, 8, 13])\n",
      "torch.Size([32965, 61])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:00<01:13,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch 0, loss_all 165.70046997070312, lr 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 12.97it/s]\n"
     ]
    }
   ],
   "source": [
    "linear_probe = torch.randn(\n",
    "    modes, model.cfg.d_model, rows, cols, one_hot_range, requires_grad=False, device=device\n",
    ")/np.sqrt(model.cfg.d_model)\n",
    "linear_probe.requires_grad = True\n",
    "print(linear_probe.shape)\n",
    "optimiser = torch.optim.AdamW([linear_probe], lr=lr, betas=(0.9, 0.99), weight_decay=wd)\n",
    "\n",
    "print(dots_indices.shape)\n",
    "# mask = dots_indices < 245\n",
    "# dots_indices = dots_indices[mask]\n",
    "\n",
    "# print(dots_indices.shape)\n",
    "\n",
    "lr = 3e-4\n",
    "max_lr = 3e-4\n",
    "min_lr = lr / 10\n",
    "max_iters = num_games * num_epochs\n",
    "decay_lr = True\n",
    "\n",
    "def get_lr(current_iter: int, max_iters: int, lr: float, min_lr: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the learning rate using linear decay.\n",
    "\n",
    "    Args:\n",
    "    - current_iter (int): The current iteration.\n",
    "    - max_iters (int): The total number of iterations for decay.\n",
    "    - lr (float): The initial learning rate.\n",
    "    - min_lr (float): The minimum learning rate after decay.\n",
    "\n",
    "    Returns:\n",
    "    - float: The calculated learning rate.\n",
    "    \"\"\"\n",
    "    # Ensure current_iter does not exceed max_iters\n",
    "    current_iter = min(current_iter, max_iters)\n",
    "\n",
    "    # Calculate the linearly decayed learning rate\n",
    "    decayed_lr = lr - (lr - min_lr) * (current_iter / max_iters)\n",
    "\n",
    "    return decayed_lr\n",
    "current_iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    full_train_indices = torch.randperm(num_games)\n",
    "    for i in tqdm(range(0, num_games, batch_size)):\n",
    "\n",
    "        lr = get_lr(current_iter, max_iters, max_lr, min_lr) if decay_lr else lr\n",
    "        for param_group in optimiser.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        indices = full_train_indices[i:i+batch_size]\n",
    "        list_of_indices = indices.tolist() # For indexing into the board_seqs_string list of strings\n",
    "        # print(list_of_indices)\n",
    "        games_int = board_seqs_int[indices]\n",
    "        games_int = games_int[:, :input_length]\n",
    "        # print(games_int.shape)\n",
    "        games_str = [board_seqs_string[idx] for idx in list_of_indices]\n",
    "        games_str = [s[:input_length] for s in games_str]\n",
    "        games_dots = dots_indices[indices]\n",
    "        games_dots = games_dots[:, 5:]\n",
    "        # print(games_dots.shape)\n",
    "        state_stack = chess_utils.create_state_stacks(games_str, custom_function)\n",
    "        # state_stack = state_stack[:, pos_start:pos_end, :, :]\n",
    "        # print(state_stack.shape)\n",
    "        # Initialize a list to hold the indexed state stacks\n",
    "        indexed_state_stacks = []\n",
    "\n",
    "        for batch_idx in range(state_stack.size(0)):\n",
    "            # Get the indices for the current batch\n",
    "            dots_indices_for_batch = games_dots[batch_idx]\n",
    "\n",
    "            # Index the state_stack for the current batch\n",
    "            indexed_state_stack = state_stack[:, batch_idx, dots_indices_for_batch, :, :]\n",
    "\n",
    "            # Append the result to the list\n",
    "            indexed_state_stacks.append(indexed_state_stack)\n",
    "\n",
    "        # Stack the indexed state stacks along the first dimension\n",
    "        # This results in a tensor of shape [2, 61, 8, 8] (assuming all batches have 61 indices)\n",
    "        state_stack = torch.stack(indexed_state_stacks)\n",
    "        # print(\"after indexing state stack shape\", state_stack.shape)\n",
    "\n",
    "        state_stack_one_hot = chess_utils.state_stack_to_one_hot(modes, rows, cols, config.min_val, config.max_val, device, state_stack).to(device)\n",
    "        with torch.inference_mode():\n",
    "            _, cache = model.run_with_cache(games_int.to(device)[:, :-1], return_type=None)\n",
    "            resid_post = cache[\"resid_post\", layer][:, :]\n",
    "        # Initialize a list to hold the indexed state stacks\n",
    "        indexed_resid_posts = []\n",
    "\n",
    "        for batch_idx in range(games_dots.size(0)):\n",
    "            # Get the indices for the current batch\n",
    "            dots_indices_for_batch = games_dots[batch_idx]\n",
    "\n",
    "            # Index the state_stack for the current batch\n",
    "            indexed_resid_post = resid_post[batch_idx, dots_indices_for_batch]\n",
    "\n",
    "            # Append the result to the list\n",
    "            indexed_resid_posts.append(indexed_resid_post)\n",
    "\n",
    "        # Stack the indexed state stacks along the first dimension\n",
    "        # This results in a tensor of shape [2, 61, 8, 8] (assuming all batches have 61 indices)\n",
    "        resid_post = torch.stack(indexed_resid_posts)\n",
    "        # print(\"Resid post\", resid_post.shape)\n",
    "        probe_out = einsum(\n",
    "            \"batch pos d_model, modes d_model rows cols options -> modes batch pos rows cols options\",\n",
    "            resid_post,\n",
    "            linear_probe,\n",
    "        )\n",
    "        # print(probe_out.shape)\n",
    "\n",
    "        # acc_blank = (probe_out[0].argmax(-1) == state_stack_one_hot[0].argmax(-1)).float().mean()\n",
    "        # acc_color = ((probe_out[1].argmax(-1) == state_stack_one_hot[1].argmax(-1)) * state_stack_one_hot[1].sum(-1)).float().sum()/(state_stack_one_hot[1]).float().sum()\n",
    "\n",
    "        \n",
    "\n",
    "        probe_log_probs = probe_out.log_softmax(-1)\n",
    "        probe_correct_log_probs = einops.reduce(\n",
    "            probe_log_probs * state_stack_one_hot,\n",
    "            \"modes batch pos rows cols options -> modes pos rows cols\",\n",
    "            \"mean\"\n",
    "        ) * one_hot_range # Multiply to correct for the mean over options\n",
    "        # loss_even = -probe_correct_log_probs[0, 0::2].mean(0).sum() # note that \"even\" means odd in the game framing, since we offset by 5 moves lol\n",
    "        # loss_odd = -probe_correct_log_probs[1, 1::2].mean(0).sum()\n",
    "        loss_all = -probe_correct_log_probs[0, :].mean(0).sum()\n",
    "\n",
    "        # if i % 1000 == 0:\n",
    "        #     print(f\"epoch {epoch}, batch {i}, acc_blank {acc_blank}, acc_color {acc_color}, loss_even {loss_even}, loss_odd {loss_odd}, loss_all {loss_all}\")\n",
    "        if i % 100 == 0:\n",
    "            print(f\"epoch {epoch}, batch {i}, loss_all {loss_all}, lr {lr}\")\n",
    "        # loss = loss_even + loss_odd + loss_all\n",
    "        loss = loss_all\n",
    "        loss.backward() # it's important to do a single backward pass for mysterious PyTorch reasons, so we add up the losses - it's per mode and per square.\n",
    "\n",
    "        optimiser.step()\n",
    "        optimiser.zero_grad()\n",
    "        current_iter += batch_size\n",
    "torch.save(linear_probe, config.linear_probe_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
