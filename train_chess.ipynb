{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens.utils as utils\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "import einops\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from fancy_einsum import einsum\n",
    "import chess\n",
    "import numpy as np\n",
    "import csv\n",
    "from dataclasses import dataclass\n",
    "from torch.nn import MSELoss, L1Loss\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import chess_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flags to control logging\n",
    "debug_mode = True\n",
    "info_mode = True\n",
    "\n",
    "if debug_mode:\n",
    "    log_level = logging.DEBUG\n",
    "elif info_mode:\n",
    "    log_level = logging.INFO\n",
    "else:\n",
    "    log_level = logging.WARNING\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=log_level)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"models/\"\n",
    "DATA_DIR = \"data/\"\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    min_val: int\n",
    "    max_val: int\n",
    "    custom_function: callable\n",
    "    linear_probe_name: str\n",
    "    num_rows: int = 8\n",
    "    num_cols: int = 8\n",
    "    board_seqs_int_file: str = f\"{DATA_DIR}train_board_seqs_int.npy\"\n",
    "    board_seqs_str_file: str = f\"{DATA_DIR}train_board_seqs_str.csv\"\n",
    "    df_file: str = f\"{DATA_DIR}train.csv\"\n",
    "    skill_file: str = None\n",
    "\n",
    "piece_config = Config(\n",
    "    min_val = -6,\n",
    "    max_val = 6,\n",
    "    custom_function = chess_utils.board_to_piece_state,\n",
    "    linear_probe_name = \"chess_piece_probe\",\n",
    ")\n",
    "\n",
    "color_config = Config(\n",
    "    min_val = -1,\n",
    "    max_val = 1,\n",
    "    custom_function=chess_utils.board_to_piece_color_state,\n",
    "    linear_probe_name=\"chess_color_probe\",\n",
    ")\n",
    "\n",
    "random_config = Config(\n",
    "    min_val = -1,\n",
    "    max_val = 1,\n",
    "    custom_function=chess_utils.board_to_random_state,\n",
    "    linear_probe_name=\"chess_random_probe\",\n",
    ")\n",
    "\n",
    "skill_config = Config(\n",
    "    min_val = -2,\n",
    "    max_val = 20,\n",
    "    custom_function=chess_utils.board_to_skill_state,\n",
    "    linear_probe_name=\"chess_skill_probe\",\n",
    "    num_rows = 1,\n",
    "    num_cols= 1,\n",
    "    board_seqs_int_file = f\"{DATA_DIR}skill_train_board_seqs_int.npy\",\n",
    "    board_seqs_str_file = f\"{DATA_DIR}skill_train_board_seqs_str.csv\",\n",
    "    df_file = f\"{DATA_DIR}skill_train.csv\",\n",
    "    skill_file = f\"{DATA_DIR}skill_train_skill_level.npy\",\n",
    ")\n",
    "\n",
    "config = piece_config\n",
    "# config = color_config\n",
    "# config = random_config\n",
    "config = skill_config\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# device = \"cpu\" # For debugging\n",
    "\n",
    "n_layers = 16\n",
    "n_heads = 8\n",
    "\n",
    "PROCESS_DATA = True\n",
    "levels_of_interest = [0, 5, 10, 20] # NOTE: This is only used if PROCESS_DATA is True\n",
    "TRAIN_WITH_MSE = True\n",
    "NORMALIZE_SKILL_FOR_MSE = True\n",
    "wandb_logging = False\n",
    "os.environ[\"WANDB_MODE\"] = \"online\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = HookedTransformerConfig(\n",
    "    n_layers = n_layers,\n",
    "    d_model = 512,\n",
    "    d_head = 64,\n",
    "    n_heads = n_heads,\n",
    "    d_mlp = 2048,\n",
    "    d_vocab = 32,\n",
    "    n_ctx = 1023,\n",
    "    act_fn=\"gelu\",\n",
    "    normalization_type=\"LNPre\"\n",
    ")\n",
    "model = HookedTransformer(cfg)\n",
    "model_name = f\"tf_lens_{n_layers}\"\n",
    "model.load_state_dict(torch.load(f'{MODEL_DIR}{model_name}.pth'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta is used to encode the string pgn strings into integer sequences\n",
    "with open(\"nanogpt/out/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "\n",
    "logger.info(meta)\n",
    "\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])\n",
    "\n",
    "logger.info(encode(\";1.e4 \"))\n",
    "logger.info(decode(encode(\";1.e4 \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df_filename = config.df_file\n",
    "processing_df_filename = f\"{DATA_DIR}temporary_in_processing.csv\"\n",
    "\n",
    "df = pd.read_csv(input_df_filename)\n",
    "df.to_csv(processing_df_filename, index=False)\n",
    "user_state_dict_one_hot_mapping = None\n",
    "\n",
    "if PROCESS_DATA:\n",
    "    df = pd.read_csv(processing_df_filename)\n",
    "    user_state_dict_one_hot_mapping = {}\n",
    "    for i in range(len(levels_of_interest)):\n",
    "        user_state_dict_one_hot_mapping[levels_of_interest[i]] = i\n",
    "\n",
    "    matches = {f'Stockfish {number}' for number in levels_of_interest}\n",
    "    logger.info(f\"Stockfish levels to be used in probe dataset: {matches}\")\n",
    "    \n",
    "    # Filter the DataFrame based on these matches\n",
    "    filtered_df = df[df['player_two'].isin(matches)]\n",
    "    filtered_df.to_csv(processing_df_filename, index=False)\n",
    "    logger.info(f\"Number of games in filtered dataset: {len(filtered_df)}\")\n",
    "\n",
    "df = pd.read_csv(processing_df_filename)\n",
    "\n",
    "\n",
    "\n",
    "prefix = \"\"\n",
    "split = \"train_\"\n",
    "if \"test\" in input_df_filename:\n",
    "    split = \"test_\"\n",
    "\n",
    "if \"skill\" in input_df_filename:\n",
    "    prefix = \"skill_\"\n",
    "\n",
    "df = pd.read_csv(f\"{processing_df_filename}\")\n",
    "row_length = len(df[\"transcript\"].iloc[0])\n",
    "num_games = len(df)\n",
    "\n",
    "assert all(\n",
    "    df[\"transcript\"].apply(lambda x: len(x) == row_length)\n",
    "), \"Not all transcripts are of length {}\".format(row_length)\n",
    "\n",
    "df[\"transcript\"].to_csv(\n",
    "    config.board_seqs_str_file, index=False, header=False\n",
    ")\n",
    "\n",
    "logger.info(f'Number of games: {len(df)},length of a game in chars: {len(df[\"transcript\"].iloc[0])}')\n",
    "\n",
    "assert (len(df), len(df[\"transcript\"].iloc[0])) == (num_games, row_length)\n",
    "\n",
    "df = pd.read_csv(processing_df_filename)\n",
    "encoded_df = df[\"transcript\"].apply(encode)\n",
    "logger.info(encoded_df.head())\n",
    "board_seqs_int = np.array(encoded_df.apply(list).tolist())\n",
    "logger.info(f\"board_seqs_int shape: {board_seqs_int.shape}\")\n",
    "assert board_seqs_int.shape == (num_games, row_length)\n",
    "\n",
    "np.save(config.board_seqs_int_file, board_seqs_int)\n",
    "\n",
    "if prefix == \"skill_\":\n",
    "    df = pd.read_csv(f\"{processing_df_filename}\")\n",
    "    # Extract skill levels as integers\n",
    "    skill_levels_list = [int(x.split()[1]) for x in df[\"player_two\"]]\n",
    "\n",
    "    # Convert the list to a numpy array\n",
    "    skill_level = np.array(skill_levels_list)\n",
    "    logger.info(f\"skill_level shape: {skill_level.shape}\")\n",
    "    assert skill_level.shape == (num_games,)\n",
    "    np.save(config.skill_file, skill_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_indexing_function = chess_utils.find_even_spaces_indices\n",
    "indexing_function_name = custom_indexing_function.__name__\n",
    "\n",
    "board_seqs_int = torch.tensor(np.load(config.board_seqs_int_file)).long()\n",
    "logger.info(f\"board_seqs_int shape: {board_seqs_int.shape}\")\n",
    "\n",
    "board_seqs_string = []\n",
    "\n",
    "with open(config.board_seqs_str_file, newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        board_seqs_string.append(row[0])\n",
    "logger.info(f\"Number of games in board_seqs_string: {len(board_seqs_string)}, length of game in chars: {len(board_seqs_string[0])}\")\n",
    "# logger.debug(board_seqs_string[0])\n",
    "\n",
    "\n",
    "custom_indices = chess_utils.find_custom_indices(processing_df_filename, custom_indexing_function)\n",
    "custom_indices = torch.tensor(custom_indices).long()\n",
    "# state_stack = torch.tensor(np.load(\"state_stacks_5k.npy\")).long() # TODO: Does loading state stack to memory speed up training?\n",
    "# logger.debug(state_stack.shape)\n",
    "logger.info(f\"custom_indices shape: {custom_indices.shape}\")\n",
    "\n",
    "num_games, shortest_length = custom_indices.shape\n",
    "num_games, game_length = board_seqs_int.shape\n",
    "assert num_games == len(board_seqs_string)\n",
    "assert game_length == len(board_seqs_string[0])\n",
    "assert num_games, shortest_length == custom_indices.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_function = config.custom_function\n",
    "skill_stack = None\n",
    "test_skill = None\n",
    "std = 1 # Default value if NORMALIZE_SKILL_FOR_MSE is False\n",
    "\n",
    "if config.skill_file is not None:\n",
    "    skill_stack = torch.tensor(np.load(config.skill_file)).long()\n",
    "    # skill_stack = (skill_stack >= 10).long() # This line can be used for binning the skill levels\n",
    "    logger.info(f\"Unique values in skill_stack: {skill_stack.unique()}\")\n",
    "    logger.info(f\"skill_stack shape: {skill_stack.shape}\")\n",
    "    test_skill = skill_stack[0]\n",
    "\n",
    "    if TRAIN_WITH_MSE and NORMALIZE_SKILL_FOR_MSE:\n",
    "        skill_stack = skill_stack.to(dtype=torch.float32) # necessary for mean and std to be float32\n",
    "        mean = skill_stack.mean()\n",
    "        std = skill_stack.std()\n",
    "\n",
    "        # Normalize the target values\n",
    "        skill_stack = (skill_stack - mean) / std\n",
    "\n",
    "\n",
    "state_stack = torch.tensor(chess_utils.create_state_stack(board_seqs_string[0], custom_function, test_skill)).long()\n",
    "logger.info(f\"A single state_stack shape: {state_stack.shape}\")\n",
    "\n",
    "state_stacks = chess_utils.create_state_stacks(board_seqs_string[:50], custom_function, skill_stack)\n",
    "logger.info(f\"state_stack shape: {state_stacks.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "wd = 0.01\n",
    "pos_start = 25 # indexes into custom_indexing_function. Example: for find_dots_indices, selects everything after the first 25 moves\n",
    "# pos_end = model.cfg.n_ctx - 5\n",
    "# input_length = 680\n",
    "# pos_end = input_length - 0\n",
    "# length = pos_end - pos_start\n",
    "one_hot_range = config.max_val - config.min_val + 1\n",
    "if PROCESS_DATA:\n",
    "    one_hot_range = len(levels_of_interest)\n",
    "num_epochs = 6\n",
    "num_games = (len(board_seqs_int) // batch_size) * batch_size # Unfortunately, num_games must be divisible by batch_size TODO: Fix this\n",
    "modes = 1\n",
    "\n",
    "max_lr = 3e-4\n",
    "min_lr = max_lr / 10\n",
    "max_iters = num_games * num_epochs\n",
    "decay_lr = True\n",
    "\n",
    "\n",
    "state_stack_one_hot = chess_utils.state_stack_to_one_hot(modes, config.num_rows, config.num_cols, config.min_val, config.max_val, device, state_stacks, user_state_dict_one_hot_mapping)\n",
    "logger.info(f\"state_stack_one_hot shape: {state_stack_one_hot.shape}\\n\")\n",
    "logger.info(f\"Note: This will only be meaningful if training on board state: \\n{state_stack_one_hot[:, 1, 170, 4:9, 2:5]}\")\n",
    "logger.info(f\"Note: This will only be meaningful if training on board state: \\n{state_stacks[:, 1, 170, 4:9, 2:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN_WITH_MSE:\n",
    "\n",
    "    def get_lr(current_iter: int, max_iters: int, max_lr: float, min_lr: float) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the learning rate using linear decay.\n",
    "\n",
    "        Args:\n",
    "        - current_iter (int): The current iteration.\n",
    "        - max_iters (int): The total number of iterations for decay.\n",
    "        - lr (float): The initial learning rate.\n",
    "        - min_lr (float): The minimum learning rate after decay.\n",
    "\n",
    "        Returns:\n",
    "        - float: The calculated learning rate.\n",
    "        \"\"\"\n",
    "        # Ensure current_iter does not exceed max_iters\n",
    "        current_iter = min(current_iter, max_iters)\n",
    "\n",
    "        # Calculate the linearly decayed learning rate\n",
    "        decayed_lr = max_lr - (max_lr - min_lr) * (current_iter / max_iters)\n",
    "\n",
    "        return decayed_lr\n",
    "\n",
    "    def train_linear_probe(layer: int):\n",
    "        linear_probe_name = f\"{MODEL_DIR}{model_name}_{config.linear_probe_name}_layer_{layer}.pth\"\n",
    "        linear_probe = torch.randn(\n",
    "            modes, model.cfg.d_model, config.num_rows, config.num_cols, one_hot_range, requires_grad=False, device=device\n",
    "        )/np.sqrt(model.cfg.d_model)\n",
    "        linear_probe.requires_grad = True\n",
    "        logger.info(f\"linear_probe shape: {linear_probe.shape}\")\n",
    "        lr = max_lr\n",
    "        optimiser = torch.optim.AdamW([linear_probe], lr=lr, betas=(0.9, 0.99), weight_decay=wd)\n",
    "\n",
    "        logger.info(f\"custom_indices shape: {custom_indices.shape}\")\n",
    "\n",
    "        # logger.debug(dots_indices.shape)\n",
    "\n",
    "        wandb_project = \"chess_linear_probes\"\n",
    "        wandb_run_name = f\"{config.linear_probe_name}_{model_name}_layer_{layer}_indexing_{indexing_function_name}\"\n",
    "\n",
    "        if wandb_logging:\n",
    "            import wandb\n",
    "            logging_dict = {\"linear_probe_name\": config.linear_probe_name, \"model_name\": model_name, \"layer\": layer,\n",
    "                            \"indexing_function_name\": indexing_function_name,\n",
    "                            \"batch_size\": batch_size, \"max_lr\": max_lr, \"wd\": wd, \"pos_start\": pos_start,\n",
    "                            \"num_epochs\": num_epochs, \"num_games\": num_games, \"modes\": modes,\n",
    "                            \"one_hot_range\": one_hot_range, \"wandb_project\": wandb_project, \"wandb_run_name\": wandb_run_name}\n",
    "            wandb.init(project=wandb_project, name=wandb_run_name, config=logging_dict)\n",
    "\n",
    "\n",
    "        current_iter = 0\n",
    "        loss = 0\n",
    "        accuracy = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            full_train_indices = torch.randperm(num_games)\n",
    "            for i in tqdm(range(0, num_games, batch_size)):\n",
    "\n",
    "                lr = get_lr(current_iter, max_iters, max_lr, min_lr) if decay_lr else lr\n",
    "                for param_group in optimiser.param_groups:\n",
    "                    param_group['lr'] = lr\n",
    "                \n",
    "                indices = full_train_indices[i:i+batch_size]\n",
    "                list_of_indices = indices.tolist() # For indexing into the board_seqs_string list of strings\n",
    "                # logger.debug(list_of_indices)\n",
    "                games_int = board_seqs_int[indices]\n",
    "                games_int = games_int[:, :]\n",
    "                # logger.debug(games_int.shape)\n",
    "                games_str = [board_seqs_string[idx] for idx in list_of_indices]\n",
    "                games_str = [s[:] for s in games_str]\n",
    "                games_dots = custom_indices[indices]\n",
    "                games_dots = games_dots[:, pos_start:]\n",
    "                # logger.debug(games_dots.shape)\n",
    "\n",
    "                if config.skill_file is not None:\n",
    "                    games_skill = skill_stack[indices]\n",
    "                    # logger.debug(games_skill.shape)\n",
    "                else:\n",
    "                    games_skill = None\n",
    "\n",
    "                state_stack = chess_utils.create_state_stacks(games_str, custom_function, games_skill)\n",
    "                # state_stack = state_stack[:, pos_start:pos_end, :, :]\n",
    "                # logger.debug(state_stack.shape)\n",
    "                # Initialize a list to hold the indexed state stacks\n",
    "                indexed_state_stacks = []\n",
    "\n",
    "                for batch_idx in range(batch_size):\n",
    "                    # Get the indices for the current batch\n",
    "                    dots_indices_for_batch = games_dots[batch_idx]\n",
    "\n",
    "                    # Index the state_stack for the current batch\n",
    "                    indexed_state_stack = state_stack[:, batch_idx, dots_indices_for_batch, :, :]\n",
    "\n",
    "                    # Append the result to the list\n",
    "                    indexed_state_stacks.append(indexed_state_stack)\n",
    "\n",
    "                # Stack the indexed state stacks along the first dimension\n",
    "                # This results in a tensor of shape [2, 61, 8, 8] (assuming all batches have 61 indices)\n",
    "                state_stack = torch.stack(indexed_state_stacks)\n",
    "                \n",
    "                # Use einops to rearrange the dimensions after stacking\n",
    "                state_stack = einops.rearrange(state_stack, 'batch modes pos row col -> modes batch pos row col')\n",
    "\n",
    "                # logger.debug(\"after indexing state stack shape\", state_stack.shape)\n",
    "\n",
    "                # state_stack_one_hot = chess_utils.state_stack_to_one_hot(modes, config.num_rows, config.num_cols, config.min_val, config.max_val, device, state_stack).to(device)\n",
    "                state_stack_one_hot = chess_utils.state_stack_to_one_hot(modes, config.num_rows, config.num_cols, 1, len(levels_of_interest), device, state_stack, user_state_dict_one_hot_mapping).to(device)\n",
    "                # logger.debug(state_stack_one_hot.shape)\n",
    "                with torch.inference_mode():\n",
    "                    _, cache = model.run_with_cache(games_int.to(device)[:, :-1], return_type=None)\n",
    "                    resid_post = cache[\"resid_post\", layer][:, :]\n",
    "                # Initialize a list to hold the indexed state stacks\n",
    "                indexed_resid_posts = []\n",
    "\n",
    "                for batch_idx in range(games_dots.size(0)):\n",
    "                    # Get the indices for the current batch\n",
    "                    dots_indices_for_batch = games_dots[batch_idx]\n",
    "\n",
    "                    # Index the state_stack for the current batch\n",
    "                    indexed_resid_post = resid_post[batch_idx, dots_indices_for_batch]\n",
    "\n",
    "                    # Append the result to the list\n",
    "                    indexed_resid_posts.append(indexed_resid_post)\n",
    "\n",
    "                # Stack the indexed state stacks along the first dimension\n",
    "                # This results in a tensor of shape [2, 61, 8, 8] (assuming all batches have 61 indices)\n",
    "                resid_post = torch.stack(indexed_resid_posts)\n",
    "                # logger.debug(\"Resid post\", resid_post.shape)\n",
    "                probe_out = einsum(\n",
    "                    \"batch pos d_model, modes d_model rows cols options -> modes batch pos rows cols options\",\n",
    "                    resid_post,\n",
    "                    linear_probe,\n",
    "                )\n",
    "                # logger.debug(probe_out.shape, state_stack_one_hot.shape, state_stack.shape)\n",
    "\n",
    "                assert probe_out.shape == state_stack_one_hot.shape\n",
    "\n",
    "                accuracy = (probe_out[0].argmax(-1) == state_stack_one_hot[0].argmax(-1)).float().mean()\n",
    "                \n",
    "\n",
    "                probe_log_probs = probe_out.log_softmax(-1)\n",
    "                probe_correct_log_probs = einops.reduce(\n",
    "                    probe_log_probs * state_stack_one_hot,\n",
    "                    \"modes batch pos rows cols options -> modes pos rows cols\",\n",
    "                    \"mean\"\n",
    "                ) * one_hot_range # Multiply to correct for the mean over one_hot_range\n",
    "                loss = -probe_correct_log_probs[0, :].mean(0).sum()\n",
    "\n",
    "                loss.backward()\n",
    "                if i % 100 == 0:\n",
    "                    logger.info(f\"epoch {epoch}, batch {i}, acc {accuracy}, loss {loss}, lr {lr}\")\n",
    "                    if wandb_logging:\n",
    "                        wandb.log({\"acc\": accuracy,\n",
    "                                \"loss\": loss,\n",
    "                                \"lr\": lr,\n",
    "                                \"epoch\": epoch,\n",
    "                                \"iter\": current_iter})\n",
    "\n",
    "                optimiser.step()\n",
    "                optimiser.zero_grad()\n",
    "                current_iter += batch_size\n",
    "\n",
    "        checkpoint = {\n",
    "            \"linear_probe\": linear_probe,\n",
    "            \"layer\": layer,\n",
    "            \"config_name\": config.linear_probe_name,\n",
    "            \"final_loss\": loss,\n",
    "            \"model_name\": model_name,\n",
    "            \"iters\": current_iter,\n",
    "            \"epochs\": epoch,\n",
    "            \"acc\": accuracy,\n",
    "        }\n",
    "        torch.save(checkpoint, linear_probe_name)\n",
    "\n",
    "    for i in range(0, n_layers, 2):\n",
    "        train_linear_probe(i)\n",
    "    # train_linear_probe(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_WITH_MSE:\n",
    "\n",
    "    mse_loss_function = MSELoss()\n",
    "    mae_loss_function = L1Loss()\n",
    "\n",
    "    def get_lr(current_iter: int, max_iters: int, max_lr: float, min_lr: float) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the learning rate using linear decay.\n",
    "\n",
    "        Args:\n",
    "        - current_iter (int): The current iteration.\n",
    "        - max_iters (int): The total number of iterations for decay.\n",
    "        - lr (float): The initial learning rate.\n",
    "        - min_lr (float): The minimum learning rate after decay.\n",
    "\n",
    "        Returns:\n",
    "        - float: The calculated learning rate.\n",
    "        \"\"\"\n",
    "        # Ensure current_iter does not exceed max_iters\n",
    "        current_iter = min(current_iter, max_iters)\n",
    "\n",
    "        # Calculate the linearly decayed learning rate\n",
    "        decayed_lr = max_lr - (max_lr - min_lr) * (current_iter / max_iters)\n",
    "\n",
    "        return decayed_lr\n",
    "\n",
    "    def train_linear_probe(layer: int):\n",
    "        linear_probe_name = f\"{MODEL_DIR}{model_name}_{config.linear_probe_name}_layer_{layer}.pth\"\n",
    "        linear_probe = torch.randn(\n",
    "            modes, model.cfg.d_model, config.num_rows, config.num_cols, requires_grad=False, device=device\n",
    "        )/np.sqrt(model.cfg.d_model)\n",
    "        linear_probe.requires_grad = True\n",
    "        logger.info(f\"linear_probe shape: {linear_probe.shape}\")\n",
    "        lr = max_lr\n",
    "        optimiser = torch.optim.AdamW([linear_probe], lr=lr, betas=(0.9, 0.99), weight_decay=wd)\n",
    "\n",
    "        logger.info(f\"custom_indices shape: {custom_indices.shape}\")\n",
    "\n",
    "        # logger.debug(dots_indices.shape)\n",
    "\n",
    "        wandb_project = \"chess_linear_probes_mse\"\n",
    "        wandb_run_name = f\"{config.linear_probe_name}_{model_name}_layer_{layer}_indexing_{indexing_function_name}\"\n",
    "\n",
    "        if wandb_logging:\n",
    "            import wandb\n",
    "            logging_dict = {\"linear_probe_name\": config.linear_probe_name, \"model_name\": model_name, \"layer\": layer,\n",
    "                            \"indexing_function_name\": indexing_function_name,\n",
    "                            \"batch_size\": batch_size, \"max_lr\": max_lr, \"wd\": wd, \"pos_start\": pos_start,\n",
    "                            \"num_epochs\": num_epochs, \"num_games\": num_games, \"modes\": modes,\n",
    "                            \"one_hot_range\": one_hot_range, \"wandb_project\": wandb_project, \"wandb_run_name\": wandb_run_name}\n",
    "            wandb.init(project=wandb_project, name=wandb_run_name, config=logging_dict)\n",
    "\n",
    "\n",
    "        current_iter = 0\n",
    "        loss = 0\n",
    "        acc_blank = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            full_train_indices = torch.randperm(num_games)\n",
    "            for i in tqdm(range(0, num_games, batch_size)):\n",
    "\n",
    "                lr = get_lr(current_iter, max_iters, max_lr, min_lr) if decay_lr else lr\n",
    "                for param_group in optimiser.param_groups:\n",
    "                    param_group['lr'] = lr\n",
    "                \n",
    "                indices = full_train_indices[i:i+batch_size]\n",
    "                # logger.debug(\"indices\", indices)\n",
    "                list_of_indices = indices.tolist() # For indexing into the board_seqs_string list of strings\n",
    "                # logger.debug(list_of_indices)\n",
    "                games_int = board_seqs_int[indices]\n",
    "                games_int = games_int[:, :]\n",
    "                # logger.debug(games_int.shape)\n",
    "                games_str = [board_seqs_string[idx] for idx in list_of_indices]\n",
    "                games_str = [s[:] for s in games_str]\n",
    "                # logger.debug(games_str)\n",
    "                games_dots = custom_indices[indices]\n",
    "                games_dots = games_dots[:, pos_start:]\n",
    "                # logger.debug(games_dots)\n",
    "                # logger.debug(games_dots.shape)\n",
    "\n",
    "                if config.skill_file is not None:\n",
    "                    games_skill = skill_stack[indices]\n",
    "                    # logger.debug(\"GAMES SKILL\", games_skill)\n",
    "                    # logger.debug(games_skill.shape)\n",
    "                else:\n",
    "                    games_skill = None\n",
    "\n",
    "                # logger.debug(\"Games skill\", games_skill)\n",
    "\n",
    "                state_stack = chess_utils.create_state_stacks(games_str, custom_function, games_skill)\n",
    "                # logger.debug(\"STATE STACK\", state_stack)\n",
    "                # state_stack = state_stack[:, pos_start:pos_end, :, :]\n",
    "                # logger.debug(\"Shape before indexing state stack:\", state_stack.shape)\n",
    "                # Initialize a list to hold the indexed state stacks\n",
    "                indexed_state_stacks = []\n",
    "\n",
    "                for batch_idx in range(batch_size): # TODO FIX Batching\n",
    "                    # Get the indices for the current batch\n",
    "                    dots_indices_for_batch = games_dots[batch_idx]\n",
    "\n",
    "                    # Index the state_stack for the current batch\n",
    "                    indexed_state_stack = state_stack[:, batch_idx, dots_indices_for_batch, :, :]\n",
    "\n",
    "                    # Append the result to the list\n",
    "                    indexed_state_stacks.append(indexed_state_stack)\n",
    "\n",
    "                # Stack the indexed state stacks along the first dimension\n",
    "                # This results in a tensor of shape [2, 61, 8, 8] (assuming all batches have 61 indices)\n",
    "                # logger.debug(\"Length of indexed state stacks\", len(indexed_state_stacks))\n",
    "                state_stack = torch.stack(indexed_state_stacks).to(device)\n",
    "                state_stack = state_stack.to(dtype=torch.float32)\n",
    "                \n",
    "                # Use einops to rearrange the dimensions after stacking\n",
    "                state_stack = einops.rearrange(state_stack, 'batch modes pos row col -> modes batch pos row col')\n",
    "\n",
    "                # logger.debug(\"state stack\", state_stack)\n",
    "\n",
    "                # logger.debug(\"after indexing state stack shape\", state_stack.shape)\n",
    "\n",
    "                with torch.inference_mode():\n",
    "                    _, cache = model.run_with_cache(games_int.to(device)[:, :-1], return_type=None)\n",
    "                    resid_post = cache[\"resid_post\", layer][:, :]\n",
    "                # Initialize a list to hold the indexed state stacks\n",
    "                indexed_resid_posts = []\n",
    "\n",
    "                for batch_idx in range(games_dots.size(0)):\n",
    "                    # Get the indices for the current batch\n",
    "                    dots_indices_for_batch = games_dots[batch_idx]\n",
    "\n",
    "                    # Index the state_stack for the current batch\n",
    "                    indexed_resid_post = resid_post[batch_idx, dots_indices_for_batch]\n",
    "\n",
    "                    # Append the result to the list\n",
    "                    indexed_resid_posts.append(indexed_resid_post)\n",
    "\n",
    "                # Stack the indexed state stacks along the first dimension\n",
    "                # This results in a tensor of shape [2, 61, 8, 8] (assuming all batches have 61 indices)\n",
    "                resid_post = torch.stack(indexed_resid_posts)\n",
    "                # logger.debug(\"Resid post\", resid_post.shape)\n",
    "                probe_out = einsum(\n",
    "                    \"batch pos d_model, modes d_model rows cols -> modes batch pos rows cols\",\n",
    "                    resid_post,\n",
    "                    linear_probe,\n",
    "                )\n",
    "                # logger.debug(probe_out.shape)\n",
    "\n",
    "                assert probe_out.shape == state_stack.shape\n",
    "\n",
    "                # acc_blank = (probe_out[0].argmax(-1) == state_stack_one_hot[0].argmax(-1)).float().mean()\n",
    "                # acc_color = ((probe_out[1].argmax(-1) == state_stack_one_hot[1].argmax(-1)) * state_stack_one_hot[1].sum(-1)).float().sum()/(state_stack_one_hot[1]).float().sum()\n",
    "\n",
    "                \n",
    "\n",
    "                # loss_even = -probe_correct_log_probs[0, 0::2].mean(0).sum() # note that \"even\" means odd in the game framing, since we offset by 5 moves lol\n",
    "                # loss_odd = -probe_correct_log_probs[1, 1::2].mean(0).sum()\n",
    "                # logger.debug(probe_out.shape, state_stack.shape)\n",
    "                # logger.debug(probe_out[0][0][0][0][0])\n",
    "                # logger.debug(state_stack[0][0][0][0][0])\n",
    "                mse_loss = mse_loss_function(probe_out, state_stack)\n",
    "                mae_loss = mae_loss_function(probe_out, state_stack)\n",
    "                \n",
    "                mae_loss_denormalized = mae_loss * std\n",
    "\n",
    "                # logger.debug(probe_out.shape, probe_out.dtype)\n",
    "                # logger.debug(state_stack.shape, state_stack.dtype)\n",
    "\n",
    "\n",
    "                if i % 200 == 0:\n",
    "                    logger.info(f\"epoch {epoch}, batch {i}, mae loss {mae_loss.item()}, mae denorm loss {mae_loss_denormalized.item()}, mse loss {mse_loss.item()}, lr {lr}\")\n",
    "                    if wandb_logging:\n",
    "                        wandb.log({\"acc\": mae_loss.item(),\n",
    "                                \"loss\": mse_loss.item(),\n",
    "                                \"mae_denorm_loss\": mae_loss_denormalized.item(),\n",
    "                                \"lr\": lr,\n",
    "                                \"epoch\": epoch,\n",
    "                                \"iter\": current_iter})\n",
    "\n",
    "                loss = mse_loss\n",
    "                loss.backward() # it's important to do a single backward pass for mysterious PyTorch reasons, so we add up the losses - it's per mode and per square.\n",
    "            \n",
    "\n",
    "                optimiser.step()\n",
    "                optimiser.zero_grad()\n",
    "                current_iter += batch_size\n",
    "\n",
    "        checkpoint = {\n",
    "            \"linear_probe\": linear_probe,\n",
    "            \"layer\": layer,\n",
    "            \"config_name\": config.linear_probe_name,\n",
    "            \"final_loss\": loss,\n",
    "            \"model_name\": model_name,\n",
    "            \"iters\": current_iter,\n",
    "            \"epochs\": epoch,\n",
    "            \"acc\": acc_blank,\n",
    "        }\n",
    "        torch.save(checkpoint, linear_probe_name)\n",
    "\n",
    "    for i in range(0, n_layers, 2):\n",
    "        train_linear_probe(i)\n",
    "    # train_linear_probe(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
