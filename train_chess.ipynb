{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-15): 16 x TransformerBlock(\n",
       "      (ln1): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformer_lens.utils as utils\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "# from mech_interp_othello_utils import OthelloBoardState\n",
    "import einops\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from fancy_einsum import einsum\n",
    "import chess\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "device = \"cuda\"\n",
    "# device = \"cpu\"\n",
    "\n",
    "n_layers = 16\n",
    "n_heads = 8\n",
    "MODEL_DIR = \"models/\"\n",
    "DATA_DIR = \"data/\"\n",
    "cfg = HookedTransformerConfig(\n",
    "    n_layers = n_layers,\n",
    "    d_model = 512,\n",
    "    d_head = 64,\n",
    "    n_heads = n_heads,\n",
    "    d_mlp = 2048,\n",
    "    d_vocab = 32,\n",
    "    n_ctx = 1023,\n",
    "    act_fn=\"gelu\",\n",
    "    normalization_type=\"LNPre\"\n",
    ")\n",
    "model = HookedTransformer(cfg)\n",
    "model.load_state_dict(torch.load(f'{MODEL_DIR}tf_lens_16.pth'))\n",
    "model.to(device)\n",
    "\n",
    "# sd = utils.download_file_from_hf(\n",
    "#     \"NeelNanda/Othello-GPT-Transformer-Lens\", \"synthetic_model.pth\"\n",
    "# )\n",
    "# # champion_ship_sd = utils.download_file_from_hf(\"NeelNanda/Othello-GPT-Transformer-Lens\", \"championship_model.pth\")\n",
    "# model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(board_seqs_int.shape)\n",
    "# # print(board_seqs_string.shape)\n",
    "# dots_indices = torch.tensor(np.load(\"dots_indices.npy\")).long()\n",
    "# state_stacks = torch.tensor(np.load(\"state_stacks.npy\")).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'board_seqs_int.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m board_seqs_int \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mboard_seqs_int.npy\u001b[39;49m\u001b[39m\"\u001b[39;49m))\u001b[39m.\u001b[39mlong()\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(board_seqs_int\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      3\u001b[0m dots_indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mdots_indices.npy\u001b[39m\u001b[39m\"\u001b[39m))\u001b[39m.\u001b[39mlong()\n",
      "File \u001b[1;32mc:\\Users\\adamk\\anaconda3\\envs\\interp\\lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'board_seqs_int.npy'"
     ]
    }
   ],
   "source": [
    "board_seqs_int = torch.tensor(np.load(f\"{DATA_DIR}board_seqs_int.npy\")).long()\n",
    "print(board_seqs_int.shape)\n",
    "dots_indices = torch.tensor(np.load(f\"{DATA_DIR}dots_indices.npy\")).long()\n",
    "# state_stack = torch.tensor(np.load(\"state_stacks_5k.npy\")).long()\n",
    "print(dots_indices.shape)\n",
    "# print(state_stack.shape)\n",
    "\n",
    "board_seqs_string = []\n",
    "\n",
    "with open(f\"{DATA_DIR}board_seqs_string.csv\", newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        board_seqs_string.append(row[0])\n",
    "print(len(board_seqs_string), len(board_seqs_string[0]))\n",
    "# print(board_seqs_string[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(680, 8, 8)\n",
      "torch.Size([680, 8, 8])\n",
      "torch.Size([50, 680, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "def pretty_print_state_stack(state: np.ndarray) -> None:\n",
    "    \"\"\"Given a state stack, print each state in a readable format.\"\"\"\n",
    "    piece_symbols = {1: 'W', -1: 'B', 0: '.'}\n",
    "\n",
    "    # Print the rows in reverse order\n",
    "    for row in reversed(state):\n",
    "        print(' '.join(piece_symbols[piece] for piece in row))\n",
    "\n",
    "def board_to_state(board: chess.Board) -> np.ndarray:\n",
    "    \"\"\"Given a chess board object, return a 8x8 np.ndarray.\n",
    "    The 8x8 array should tell if each square is black, white, or blank.\n",
    "    White is 1, black is -1, and blank is 0.\n",
    "    In the 8x8 array, row 0 is A1-H1 (White), row 1 is A2-H2, etc.\"\"\"\n",
    "    state = np.zeros((8, 8), dtype=int)\n",
    "    for i in range(64):\n",
    "        piece = board.piece_at(i)\n",
    "        if piece:\n",
    "            # Assign 1 for white pieces and -1 for black pieces\n",
    "            state[i // 8, i % 8] = 1 if piece.color == chess.WHITE else -1\n",
    "\n",
    "    return state\n",
    "\n",
    "def create_state_stack(moves_string: str) -> np.ndarray:\n",
    "    \"\"\"Given a string of PGN format moves, create an 8x8 np.ndarray for every character in the string.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    board = chess.Board()\n",
    "    initial_states = []\n",
    "    count = 1\n",
    "\n",
    "    # Scan 1: Creates states, with length = number of moves in the game\n",
    "    initial_states.append(board_to_state(board))\n",
    "    # Apply each move to the board\n",
    "    for move in moves_string.split():\n",
    "        # because all games are truncated to len 680, often the last move is partial and invalid\n",
    "        try:\n",
    "            # Skip move numbers\n",
    "            if '.' in move:\n",
    "                board.push_san(move.split(\".\")[1])\n",
    "            else:\n",
    "                board.push_san(move)\n",
    "\n",
    "            initial_states.append(board_to_state(board))\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        # For debugging and sanity checking\n",
    "        # count += 1\n",
    "        # if count % 100 == 0:\n",
    "        #     pretty_print_state_stack(board_to_state(board))\n",
    "        #     print(\"_\" * 50)\n",
    "        #     print(board)\n",
    "\n",
    "    # return np.array(initial_states)\n",
    "\n",
    "    # Second Scan: Expand states to match the length of moves_string\n",
    "    # For ;1.e4 e5 2.Nf3, \";1.e4\" = idx 0, \" e5\" = idx 1, \" 2.Nf3\" = idx 2\n",
    "    expanded_states = []\n",
    "    move_index = 0\n",
    "    for char in moves_string:\n",
    "        if char == ' ':\n",
    "            move_index += 1\n",
    "        expanded_states.append(initial_states[min(move_index, len(initial_states) - 1)])\n",
    "\n",
    "    # expanded_states.append(initial_states[-1]) # The last element in expanded_states is the final position of the board.\n",
    "    # Currently not using this as len(expanded_states) would be 1 greater than len(moves_string) and that would be confusing.\n",
    "    return np.array(expanded_states)\n",
    "\n",
    "def create_state_stacks(moves_strings: list[str]) -> torch.Tensor:\n",
    "    state_stacks = []\n",
    "\n",
    "    for board in moves_strings:\n",
    "        state_stack = torch.tensor(create_state_stack(board)).long()\n",
    "        state_stacks.append(state_stack)\n",
    "\n",
    "    # Convert the list of tensors to a single tensor\n",
    "    final_state_stack = torch.stack(state_stacks)\n",
    "    return final_state_stack\n",
    "\n",
    "# print(board_seqs_string[0])\n",
    "print(create_state_stack(board_seqs_string[0]).shape)\n",
    "\n",
    "state_stack = torch.tensor(create_state_stack(board_seqs_string[0])).long()\n",
    "print(state_stack.shape)\n",
    "\n",
    "state_stacks = create_state_stacks(board_seqs_string[:50])\n",
    "print(state_stacks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 680, 8, 8, 3])\n",
      "tensor([[[[1, 0, 0],\n",
      "          [1, 0, 0],\n",
      "          [1, 0, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [0, 0, 1],\n",
      "          [1, 0, 0]],\n",
      "\n",
      "         [[1, 0, 0],\n",
      "          [1, 0, 0],\n",
      "          [1, 0, 0]],\n",
      "\n",
      "         [[0, 1, 0],\n",
      "          [1, 0, 0],\n",
      "          [1, 0, 0]]]])\n",
      "tensor([[ 0,  0,  0],\n",
      "        [-1,  1,  0],\n",
      "        [ 0,  0,  0],\n",
      "        [-1,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "layer = 12\n",
    "batch_size = 1\n",
    "lr = 2e-4\n",
    "wd = 0.01\n",
    "pos_start = 0\n",
    "# pos_end = model.cfg.n_ctx - 5\n",
    "input_length = 680\n",
    "pos_end = input_length - 0\n",
    "length = pos_end - pos_start\n",
    "options = 3\n",
    "rows = 8\n",
    "cols = 8\n",
    "num_epochs = 2\n",
    "num_games = 20000\n",
    "x = 0\n",
    "y = 2\n",
    "probe_name = \"main_chess_linear_probe\"\n",
    "# The first mode is blank or not, the second mode is next or prev GIVEN that it is not blank\n",
    "modes = 1\n",
    "alternating = torch.tensor([1 if i%2 == 0 else -1 for i in range(length)], device=device)\n",
    "\n",
    "\n",
    "def state_stack_to_one_hot(state_stack):\n",
    "    one_hot = torch.zeros(\n",
    "        modes, # blank vs color (mode)\n",
    "        state_stack.shape[0], # num games\n",
    "        state_stack.shape[1], # num moves\n",
    "        rows, # rows\n",
    "        cols, # cols\n",
    "        options, # the two options\n",
    "        device=state_stack.device,\n",
    "        dtype=int,\n",
    "    )\n",
    "    one_hot[:, ..., 0] = state_stack == 0\n",
    "    one_hot[:, ..., 1] = state_stack == -1\n",
    "    one_hot[:, ..., 2] = state_stack == 1\n",
    "    return one_hot\n",
    "state_stack_one_hot = state_stack_to_one_hot(state_stacks)\n",
    "print(state_stack_one_hot.shape)\n",
    "print((state_stack_one_hot[:, 1, 170, 4:9, 2:5]))\n",
    "print((state_stacks[1, 170, 4:9, 2:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 8, 8, 3])\n",
      "torch.Size([41206, 61])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/20000 [00:00<53:08,  6.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch 0, loss_all 71.84343719482422, lr 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 102/20000 [00:11<35:59,  9.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch 100, loss_all 49.125587463378906, lr 0.00029932499999999997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 202/20000 [00:21<36:22,  9.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch 200, loss_all 37.547401428222656, lr 0.00029864999999999997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 302/20000 [00:30<33:07,  9.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch 300, loss_all 36.78445053100586, lr 0.00029797499999999997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 83\u001b[0m\n\u001b[1;32m     81\u001b[0m state_stack_one_hot \u001b[38;5;241m=\u001b[39m state_stack_to_one_hot(state_stack)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m---> 83\u001b[0m     _, cache \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgames_int\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     resid_post \u001b[38;5;241m=\u001b[39m cache[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresid_post\u001b[39m\u001b[38;5;124m\"\u001b[39m, layer][:, :]\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Initialize a list to hold the indexed state stacks\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/HookedTransformer.py:634\u001b[0m, in \u001b[0;36mHookedTransformer.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_with_cache\u001b[39m(\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mmodel_args, return_cache_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    619\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    626\u001b[0m     Union[ActivationCache, Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]],\n\u001b[1;32m    627\u001b[0m ]:\n\u001b[1;32m    628\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper around `run_with_cache` in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \n\u001b[1;32m    630\u001b[0m \u001b[38;5;124;03m    If return_cache_object is True, this will return an ActivationCache object, with a bunch of\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;124;03m    useful HookedTransformer specific methods, otherwise it will return a dictionary of\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;124;03m    activations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m     out, cache_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    638\u001b[0m         cache \u001b[38;5;241m=\u001b[39m ActivationCache(\n\u001b[1;32m    639\u001b[0m             cache_dict, \u001b[38;5;28mself\u001b[39m, has_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m remove_batch_dim\n\u001b[1;32m    640\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/hook_points.py:467\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m cache_dict, fwd, bwd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    458\u001b[0m     names_filter, incl_bwd, device, remove_batch_dim\u001b[38;5;241m=\u001b[39mremove_batch_dim\n\u001b[1;32m    459\u001b[0m )\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks(\n\u001b[1;32m    462\u001b[0m     fwd_hooks\u001b[38;5;241m=\u001b[39mfwd,\n\u001b[1;32m    463\u001b[0m     bwd_hooks\u001b[38;5;241m=\u001b[39mbwd,\n\u001b[1;32m    464\u001b[0m     reset_hooks_end\u001b[38;5;241m=\u001b[39mreset_hooks_end,\n\u001b[1;32m    465\u001b[0m     clear_contexts\u001b[38;5;241m=\u001b[39mclear_contexts,\n\u001b[1;32m    466\u001b[0m ):\n\u001b[0;32m--> 467\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    469\u001b[0m         model_out\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/HookedTransformer.py:555\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    551\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    552\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    553\u001b[0m         )\n\u001b[0;32m--> 555\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/components.py:1189\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m   1182\u001b[0m     key_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m   1183\u001b[0m     value_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m   1185\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_attn_out(\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1198\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mattn_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mparallel_attn_mlp:\n\u001b[1;32m   1200\u001b[0m     resid_mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_resid_mid(\n\u001b[1;32m   1201\u001b[0m         resid_pre \u001b[38;5;241m+\u001b[39m attn_out\n\u001b[1;32m   1202\u001b[0m     )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/components.py:555\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask)\u001b[0m\n\u001b[1;32m    544\u001b[0m     qkv_einops_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_q(\n\u001b[1;32m    546\u001b[0m     einsum(\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mqkv_einops_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, head_index d_model d_head \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_Q\n\u001b[1;32m    553\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, head_index, d_head]\u001b[39;00m\n\u001b[1;32m    554\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_k(\n\u001b[0;32m--> 555\u001b[0m     \u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mqkv_einops_string\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m, head_index d_model d_head \u001b[39;49m\u001b[38;5;130;43;01m\\\u001b[39;49;00m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;124;43m        -> batch pos head_index d_head\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_K\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_K\n\u001b[1;32m    562\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, head_index, d_head]\u001b[39;00m\n\u001b[1;32m    563\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_v(\n\u001b[1;32m    564\u001b[0m     einsum(\n\u001b[1;32m    565\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mqkv_einops_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, head_index d_model d_head \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_V\n\u001b[1;32m    571\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, head_index, d_head]\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_kv_cache_entry \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;66;03m# Appends the new keys and values to the cached values, and automatically updates the cache\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fancy_einsum/__init__.py:135\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluates the Einstein summation convention on the operands.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03mSee: \u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m  https://pytorch.org/docs/stable/generated/torch.einsum.html\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m  https://numpy.org/doc/stable/reference/generated/numpy.einsum.html\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    134\u001b[0m backend \u001b[38;5;241m=\u001b[39m get_backend(operands[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 135\u001b[0m new_equation \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_equation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39meinsum(new_equation, \u001b[38;5;241m*\u001b[39moperands)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/fancy_einsum/__init__.py:79\u001b[0m, in \u001b[0;36mconvert_equation\u001b[0;34m(equation)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convert an equation using human-readable variable names to an equation using single letter.\"\"\"\u001b[39;00m\n\u001b[1;32m     78\u001b[0m SPECIAL \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m->\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 79\u001b[0m terms \u001b[38;5;241m=\u001b[39m \u001b[43m_part_re\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m->\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m terms:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# Infer RHS side\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# Important that we sort alphabetically by long names, not short ones\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     rhs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m->\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "linear_probe = torch.randn(\n",
    "    modes, model.cfg.d_model, rows, cols, options, requires_grad=False, device=device\n",
    ")/np.sqrt(model.cfg.d_model)\n",
    "linear_probe.requires_grad = True\n",
    "print(linear_probe.shape)\n",
    "optimiser = torch.optim.AdamW([linear_probe], lr=lr, betas=(0.9, 0.99), weight_decay=wd)\n",
    "\n",
    "print(dots_indices.shape)\n",
    "# mask = dots_indices < 245\n",
    "# dots_indices = dots_indices[mask]\n",
    "\n",
    "# print(dots_indices.shape)\n",
    "\n",
    "lr = 3e-4\n",
    "max_lr = 3e-4\n",
    "min_lr = lr / 10\n",
    "max_iters = num_games * num_epochs\n",
    "decay_lr = True\n",
    "\n",
    "def get_lr(current_iter: int, max_iters: int, lr: float, min_lr: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the learning rate using linear decay.\n",
    "\n",
    "    Args:\n",
    "    - current_iter (int): The current iteration.\n",
    "    - max_iters (int): The total number of iterations for decay.\n",
    "    - lr (float): The initial learning rate.\n",
    "    - min_lr (float): The minimum learning rate after decay.\n",
    "\n",
    "    Returns:\n",
    "    - float: The calculated learning rate.\n",
    "    \"\"\"\n",
    "    # Ensure current_iter does not exceed max_iters\n",
    "    current_iter = min(current_iter, max_iters)\n",
    "\n",
    "    # Calculate the linearly decayed learning rate\n",
    "    decayed_lr = lr - (lr - min_lr) * (current_iter / max_iters)\n",
    "\n",
    "    return decayed_lr\n",
    "current_iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    full_train_indices = torch.randperm(num_games)\n",
    "    for i in tqdm(range(0, num_games, batch_size)):\n",
    "\n",
    "        lr = get_lr(current_iter, max_iters, max_lr, min_lr) if decay_lr else lr\n",
    "        for param_group in optimiser.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        indices = full_train_indices[i:i+batch_size]\n",
    "        list_of_indices = indices.tolist() # For indexing into the board_seqs_string list of strings\n",
    "        # print(list_of_indices)\n",
    "        games_int = board_seqs_int[indices]\n",
    "        games_int = games_int[:, :input_length]\n",
    "        # print(games_int.shape)\n",
    "        games_str = [board_seqs_string[idx] for idx in list_of_indices]\n",
    "        games_str = [s[:input_length] for s in games_str]\n",
    "        games_dots = dots_indices[indices]\n",
    "        games_dots = games_dots[:, 5:]\n",
    "        # print(games_dots.shape)\n",
    "        state_stack = create_state_stacks(games_str)\n",
    "        # state_stack = state_stack[:, pos_start:pos_end, :, :]\n",
    "        # print(state_stack.shape)\n",
    "        # Initialize a list to hold the indexed state stacks\n",
    "        indexed_state_stacks = []\n",
    "\n",
    "        for batch_idx in range(state_stack.size(0)):\n",
    "            # Get the indices for the current batch\n",
    "            dots_indices_for_batch = games_dots[batch_idx]\n",
    "\n",
    "            # Index the state_stack for the current batch\n",
    "            indexed_state_stack = state_stack[batch_idx, dots_indices_for_batch, :, :]\n",
    "\n",
    "            # Append the result to the list\n",
    "            indexed_state_stacks.append(indexed_state_stack)\n",
    "\n",
    "        # Stack the indexed state stacks along the first dimension\n",
    "        # This results in a tensor of shape [2, 61, 8, 8] (assuming all batches have 61 indices)\n",
    "        state_stack = torch.stack(indexed_state_stacks)\n",
    "        # print(\"after indexing state stack shape\", state_stack.shape)\n",
    "\n",
    "        state_stack_one_hot = state_stack_to_one_hot(state_stack).to(device)\n",
    "        with torch.inference_mode():\n",
    "            _, cache = model.run_with_cache(games_int.to(device)[:, :-1], return_type=None)\n",
    "            resid_post = cache[\"resid_post\", layer][:, :]\n",
    "        # Initialize a list to hold the indexed state stacks\n",
    "        indexed_resid_posts = []\n",
    "\n",
    "        for batch_idx in range(games_dots.size(0)):\n",
    "            # Get the indices for the current batch\n",
    "            dots_indices_for_batch = games_dots[batch_idx]\n",
    "\n",
    "            # Index the state_stack for the current batch\n",
    "            indexed_resid_post = resid_post[batch_idx, dots_indices_for_batch]\n",
    "\n",
    "            # Append the result to the list\n",
    "            indexed_resid_posts.append(indexed_resid_post)\n",
    "\n",
    "        # Stack the indexed state stacks along the first dimension\n",
    "        # This results in a tensor of shape [2, 61, 8, 8] (assuming all batches have 61 indices)\n",
    "        resid_post = torch.stack(indexed_resid_posts)\n",
    "        # print(\"Resid post\", resid_post.shape)\n",
    "        probe_out = einsum(\n",
    "            \"batch pos d_model, modes d_model rows cols options -> modes batch pos rows cols options\",\n",
    "            resid_post,\n",
    "            linear_probe,\n",
    "        )\n",
    "        # print(probe_out.shape)\n",
    "\n",
    "        # acc_blank = (probe_out[0].argmax(-1) == state_stack_one_hot[0].argmax(-1)).float().mean()\n",
    "        # acc_color = ((probe_out[1].argmax(-1) == state_stack_one_hot[1].argmax(-1)) * state_stack_one_hot[1].sum(-1)).float().sum()/(state_stack_one_hot[1]).float().sum()\n",
    "\n",
    "        \n",
    "\n",
    "        probe_log_probs = probe_out.log_softmax(-1)\n",
    "        probe_correct_log_probs = einops.reduce(\n",
    "            probe_log_probs * state_stack_one_hot,\n",
    "            \"modes batch pos rows cols options -> modes pos rows cols\",\n",
    "            \"mean\"\n",
    "        ) * options # Multiply to correct for the mean over options\n",
    "        # loss_even = -probe_correct_log_probs[0, 0::2].mean(0).sum() # note that \"even\" means odd in the game framing, since we offset by 5 moves lol\n",
    "        # loss_odd = -probe_correct_log_probs[1, 1::2].mean(0).sum()\n",
    "        loss_all = -probe_correct_log_probs[0, :].mean(0).sum()\n",
    "\n",
    "        # if i % 1000 == 0:\n",
    "        #     print(f\"epoch {epoch}, batch {i}, acc_blank {acc_blank}, acc_color {acc_color}, loss_even {loss_even}, loss_odd {loss_odd}, loss_all {loss_all}\")\n",
    "        if i % 100 == 0:\n",
    "            print(f\"epoch {epoch}, batch {i}, loss_all {loss_all}, lr {lr}\")\n",
    "        # loss = loss_even + loss_odd + loss_all\n",
    "        loss = loss_all\n",
    "        loss.backward() # it's important to do a single backward pass for mysterious PyTorch reasons, so we add up the losses - it's per mode and per square.\n",
    "\n",
    "        optimiser.step()\n",
    "        optimiser.zero_grad()\n",
    "        current_iter += batch_size\n",
    "torch.save(linear_probe, f\"{probe_name}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
