{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import einops\n",
    "\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x1fd404765f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_AND_CONVERT_CHECKPOINT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['model', 'optimizer', 'model_args', 'iter_num', 'best_val_loss', 'config'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "checkpoint = torch.load('models/ckpt_3487k_iters_pre_dropout.pt')\n",
    "\n",
    "# Print the keys of the checkpoint dictionary\n",
    "print(checkpoint.keys())\n",
    "model_state = checkpoint['model']\n",
    "# for key, value in model_state.items():\n",
    "#     print(key, value.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load('main_linear_probe.pth')\n",
    "# print(checkpoint.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformer_lens.utils as utils\n",
    "# cfg = HookedTransformerConfig(\n",
    "#     n_layers = 16,\n",
    "#     d_model = 512,\n",
    "#     d_head = 64,\n",
    "#     n_heads = 8,\n",
    "#     d_mlp = 2048,\n",
    "#     d_vocab = 61,\n",
    "#     n_ctx = 59,\n",
    "#     act_fn=\"gelu\",\n",
    "#     normalization_type=\"LNPre\"\n",
    "# )\n",
    "# model = HookedTransformer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_orig_mod.transformer.wte.weight torch.Size([32, 512])\n",
      "_orig_mod.transformer.wpe.weight torch.Size([1023, 512])\n",
      "_orig_mod.transformer.h.0.ln_1.weight torch.Size([512])\n",
      "_orig_mod.transformer.h.0.attn.c_attn.weight torch.Size([1536, 512])\n",
      "_orig_mod.transformer.h.0.attn.c_proj.weight torch.Size([512, 512])\n",
      "_orig_mod.transformer.h.0.ln_2.weight torch.Size([512])\n",
      "_orig_mod.transformer.h.0.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "_orig_mod.transformer.h.0.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "_orig_mod.transformer.ln_f.weight torch.Size([512])\n",
      "_orig_mod.lm_head.weight torch.Size([32, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([8, 512, 64])\n",
      "torch.Size([8, 512, 64])\n",
      "torch.Size([8, 512, 64])\n",
      "torch.Size([512, 512])\n",
      "torch.Size([8, 64, 512])\n",
      "tensor([[ 6,  4, 27,  9,  0, 27, 10,  0,  7,  4, 19, 28]], device='cuda:0')\n",
      "tensor([[True, True, True, True, True, True, True, True, True, True, True, True]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def convert_to_transformer_lens_format(in_sd, n_layers=16, n_heads=8):\n",
    "    out_sd = {}\n",
    "    out_sd[\"pos_embed.W_pos\"] = in_sd[\"_orig_mod.transformer.wpe.weight\"]\n",
    "    out_sd[\"embed.W_E\"] = in_sd[\"_orig_mod.transformer.wte.weight\"]\n",
    "\n",
    "    out_sd[\"ln_final.w\"] = in_sd[\"_orig_mod.transformer.ln_f.weight\"]\n",
    "    out_sd[\"ln_final.b\"] = torch.zeros_like(in_sd[\"_orig_mod.transformer.ln_f.weight\"])\n",
    "    out_sd[\"unembed.W_U\"] = in_sd[\"_orig_mod.lm_head.weight\"].T\n",
    "\n",
    "    for layer in range(n_layers):\n",
    "        layer_key = f\"_orig_mod.transformer.h.{layer}\"\n",
    "\n",
    "        # Layer Norms\n",
    "        out_sd[f\"blocks.{layer}.ln1.w\"] = in_sd[f\"{layer_key}.ln_1.weight\"]\n",
    "        out_sd[f\"blocks.{layer}.ln1.b\"] = torch.zeros_like(in_sd[f\"{layer_key}.ln_1.weight\"])\n",
    "        out_sd[f\"blocks.{layer}.ln2.w\"] = in_sd[f\"{layer_key}.ln_2.weight\"]\n",
    "        out_sd[f\"blocks.{layer}.ln2.b\"] = torch.zeros_like(in_sd[f\"{layer_key}.ln_2.weight\"])\n",
    "\n",
    "        W = in_sd[f\"{layer_key}.attn.c_attn.weight\"]\n",
    "        W_Q, W_K, W_V = torch.tensor_split(W, 3, dim=0)\n",
    "        W_Q = einops.rearrange(W_Q, \"(i h) m->i m h\", i=cfg.n_heads)\n",
    "        W_K = einops.rearrange(W_K, \"(i h) m->i m h\", i=cfg.n_heads)\n",
    "        W_V = einops.rearrange(W_V, \"(i h) m->i m h\", i=cfg.n_heads)\n",
    "        out_sd[f\"blocks.{layer}.attn.W_Q\"] = W_Q\n",
    "        out_sd[f\"blocks.{layer}.attn.W_K\"] = W_K\n",
    "        out_sd[f\"blocks.{layer}.attn.W_V\"] = W_V\n",
    "        # out_sd[f\"blocks.{layer}.attn.b_Q\"] = torch.zeros_like(W_Q)\n",
    "        # out_sd[f\"blocks.{layer}.attn.b_K\"] = torch.zeros_like(W_K)\n",
    "        # out_sd[f\"blocks.{layer}.attn.b_V\"] = torch.zeros_like(W_V)\n",
    "        W_O = in_sd[f\"{layer_key}.attn.c_proj.weight\"]\n",
    "        W_O = einops.rearrange(W_O, \"m (i h)->i h m\", i=cfg.n_heads)\n",
    "        out_sd[f\"blocks.{layer}.attn.W_O\"] = W_O\n",
    "\n",
    "        # MLP Weights\n",
    "        out_sd[f\"blocks.{layer}.mlp.W_in\"] = in_sd[f\"{layer_key}.mlp.c_fc.weight\"].T\n",
    "        # out_sd[f\"blocks.{layer}.mlp.b_in\"] = torch.zeros_like(in_sd[f\"{layer_key}.mlp.c_fc.weight\"][0])\n",
    "        out_sd[f\"blocks.{layer}.mlp.W_out\"] = in_sd[f\"{layer_key}.mlp.c_proj.weight\"].T\n",
    "        # out_sd[f\"blocks.{layer}.mlp.b_out\"] = torch.zeros_like(in_sd[f\"{layer_key}.mlp.c_proj.weight\"][0])\n",
    "\n",
    "\n",
    "    return out_sd\n",
    "\n",
    "if LOAD_AND_CONVERT_CHECKPOINT:\n",
    "\n",
    "    synthetic_checkpoint = model_state\n",
    "    for name, param in synthetic_checkpoint.items():\n",
    "        if name.startswith(\"_orig_mod.transformer.h.0\") or not name.startswith(\"_orig_mod.transformer.h\"):\n",
    "            print(name, param.shape)\n",
    "\n",
    "    n_heads = 8\n",
    "    n_layers = 16\n",
    "\n",
    "    cfg = HookedTransformerConfig(\n",
    "        n_layers = n_layers,\n",
    "        d_model = 512,\n",
    "        d_head = 64,\n",
    "        n_heads = n_heads,\n",
    "        d_mlp = 2048,\n",
    "        d_vocab = 32,\n",
    "        n_ctx = 1023,\n",
    "        act_fn=\"gelu\",\n",
    "        normalization_type=\"LNPre\"\n",
    "    )\n",
    "    model = HookedTransformer(cfg)\n",
    "\n",
    "\n",
    "    model.load_and_process_state_dict(convert_to_transformer_lens_format(synthetic_checkpoint, n_layers=n_layers, n_heads=n_heads))\n",
    "\n",
    "# An example input\n",
    "sample_input = torch.tensor([[15, 6, 4, 27, 9, 0, 25, 10, 0, 7, 4, 19]])\n",
    "# sample_input = torch.tensor([[15, 6, 4, 27, 9]])\n",
    "# The argmax of the output (ie the most likely next move from each position)\n",
    "sample_output = torch.tensor([[ 6,  4, 27,  9,  0, 27, 10,  0,  7,  4, 19, 28]]).to(\"cuda\")\n",
    "model_output = model(sample_input).argmax(dim=-1)\n",
    "print(model_output)\n",
    "print(sample_output == model_output)\n",
    "# print(model.forward(sample_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6, 4]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# An example input\n",
    "sample_input = torch.tensor([[15, 6]])\n",
    "# sample_input = torch.tensor([[15, 6, 4, 27, 9]])\n",
    "# The argmax of the output (ie the most likely next move from each position)\n",
    "sample_output = torch.tensor([[21, 41, 40, 34, 40, 41,  3, 11, 21, 43, 40, 21, 28, 50, 33, 50, 33,  5,\n",
    "         33,  5, 52, 46, 14, 46, 14, 47, 38, 57, 36, 50, 38, 15, 28, 26, 28, 59,\n",
    "         50, 28, 14, 28, 28, 28, 28, 45, 28, 35, 15, 14, 30, 59, 49, 59, 15, 15,\n",
    "         14, 15,  8,  7,  8]])\n",
    "print(model(sample_input).argmax(dim=-1))\n",
    "# print(model.forward(sample_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'tf_lens_16.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6, 4]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "n_heads = 8\n",
    "n_layers = 16\n",
    "\n",
    "cfg = HookedTransformerConfig(\n",
    "    n_layers = n_layers,\n",
    "    d_model = 512,\n",
    "    d_head = 64,\n",
    "    n_heads = n_heads,\n",
    "    d_mlp = 2048,\n",
    "    d_vocab = 32,\n",
    "    n_ctx = 1023,\n",
    "    act_fn=\"gelu\",\n",
    "    normalization_type=\"LNPre\"\n",
    ")\n",
    "model2 = HookedTransformer(cfg)\n",
    "\n",
    "model2.load_state_dict(torch.load('tf_lens_16.pth'))\n",
    "\n",
    "model_output = model2(sample_input).argmax(dim=-1)\n",
    "print(model_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
