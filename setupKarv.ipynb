{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For nanogpt to transformer lens conversion\n",
    "import torch\n",
    "import einops\n",
    "import os\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens import (\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    ")\n",
    "# Our pytorch model is in the nanogpt format. For easy linear probing of the residual stream, we want to convert\n",
    "# it to the transformer lens format. This is done in the following code block.\n",
    "# This code was developed using Neel Nanda's othello_reference/Othello_GPT.ipynb as a reference.\n",
    "\n",
    "\n",
    "\n",
    "def convert_nanogpt_weights(old_state_dict, cfg: HookedTransformerConfig, bias: bool = False):\n",
    "    \"\"\"For https://github.com/karpathy/nanoGPT\n",
    "    There are two complications with converting nanogpt models:\n",
    "    The first is that some state dicts have an unwanted prefix on keys that needs to be removed.\n",
    "    The second is that the models can be saved with or without bias. By default, there\n",
    "    is no bias. This function can handle both cases.\"\"\"\n",
    "    # Nanogpt models saved after torch.compile() have this unwanted prefix\n",
    "    # This is a simple way to remove it\n",
    "    unwanted_prefix = \"_orig_mod.\"\n",
    "    for k, v in list(old_state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            old_state_dict[k[len(unwanted_prefix) :]] = old_state_dict.pop(k)\n",
    "\n",
    "    new_state_dict = {}\n",
    "    new_state_dict[\"pos_embed.W_pos\"] = old_state_dict[\"transformer.wpe.weight\"]\n",
    "    new_state_dict[\"embed.W_E\"] = old_state_dict[\"transformer.wte.weight\"]\n",
    "\n",
    "    new_state_dict[\"ln_final.w\"] = old_state_dict[\"transformer.ln_f.weight\"]\n",
    "    new_state_dict[\"ln_final.b\"] = torch.zeros_like(old_state_dict[\"transformer.ln_f.weight\"])\n",
    "    new_state_dict[\"unembed.W_U\"] = old_state_dict[\"lm_head.weight\"].T\n",
    "\n",
    "    if bias:\n",
    "        new_state_dict[\"ln_final.b\"] = old_state_dict[\"transformer.ln_f.bias\"]\n",
    "\n",
    "    for layer in range(cfg.n_layers):\n",
    "        layer_key = f\"transformer.h.{layer}\"\n",
    "\n",
    "        new_state_dict[f\"blocks.{layer}.ln1.w\"] = old_state_dict[f\"{layer_key}.ln_1.weight\"]\n",
    "        # A bias of zeros is required for folding layer norm\n",
    "        new_state_dict[f\"blocks.{layer}.ln1.b\"] = torch.zeros_like(\n",
    "            old_state_dict[f\"{layer_key}.ln_1.weight\"]\n",
    "        )\n",
    "        new_state_dict[f\"blocks.{layer}.ln2.w\"] = old_state_dict[f\"{layer_key}.ln_2.weight\"]\n",
    "        new_state_dict[f\"blocks.{layer}.ln2.b\"] = torch.zeros_like(\n",
    "            old_state_dict[f\"{layer_key}.ln_2.weight\"]\n",
    "        )\n",
    "\n",
    "        W = old_state_dict[f\"{layer_key}.attn.c_attn.weight\"]\n",
    "        W_Q, W_K, W_V = torch.tensor_split(W, 3, dim=0)\n",
    "        W_Q = einops.rearrange(W_Q, \"(i h) m->i m h\", i=cfg.n_heads)\n",
    "        W_K = einops.rearrange(W_K, \"(i h) m->i m h\", i=cfg.n_heads)\n",
    "        W_V = einops.rearrange(W_V, \"(i h) m->i m h\", i=cfg.n_heads)\n",
    "        new_state_dict[f\"blocks.{layer}.attn.W_Q\"] = W_Q\n",
    "        new_state_dict[f\"blocks.{layer}.attn.W_K\"] = W_K\n",
    "        new_state_dict[f\"blocks.{layer}.attn.W_V\"] = W_V\n",
    "\n",
    "        W_O = old_state_dict[f\"{layer_key}.attn.c_proj.weight\"]\n",
    "        W_O = einops.rearrange(W_O, \"m (i h)->i h m\", i=cfg.n_heads)\n",
    "        new_state_dict[f\"blocks.{layer}.attn.W_O\"] = W_O\n",
    "\n",
    "        new_state_dict[f\"blocks.{layer}.mlp.W_in\"] = old_state_dict[\n",
    "            f\"{layer_key}.mlp.c_fc.weight\"\n",
    "        ].T\n",
    "        new_state_dict[f\"blocks.{layer}.mlp.W_out\"] = old_state_dict[\n",
    "            f\"{layer_key}.mlp.c_proj.weight\"\n",
    "        ].T\n",
    "\n",
    "        if bias:\n",
    "            new_state_dict[f\"blocks.{layer}.ln1.b\"] = old_state_dict[f\"{layer_key}.ln_1.bias\"]\n",
    "            new_state_dict[f\"blocks.{layer}.ln2.b\"] = old_state_dict[f\"{layer_key}.ln_2.bias\"]\n",
    "            new_state_dict[f\"blocks.{layer}.mlp.b_in\"] = old_state_dict[\n",
    "                f\"{layer_key}.mlp.c_fc.bias\"\n",
    "            ]\n",
    "            new_state_dict[f\"blocks.{layer}.mlp.b_out\"] = old_state_dict[\n",
    "                f\"{layer_key}.mlp.c_proj.bias\"\n",
    "            ]\n",
    "\n",
    "            B = old_state_dict[f\"{layer_key}.attn.c_attn.bias\"]\n",
    "            B_Q, B_K, B_V = torch.tensor_split(B, 3, dim=0)\n",
    "            B_Q = einops.rearrange(B_Q, \"(i h)->i h\", i=cfg.n_heads)\n",
    "            B_K = einops.rearrange(B_K, \"(i h)->i h\", i=cfg.n_heads)\n",
    "            B_V = einops.rearrange(B_V, \"(i h)->i h\", i=cfg.n_heads)\n",
    "            new_state_dict[f\"blocks.{layer}.attn.b_Q\"] = B_Q\n",
    "            new_state_dict[f\"blocks.{layer}.attn.b_K\"] = B_K\n",
    "            new_state_dict[f\"blocks.{layer}.attn.b_V\"] = B_V\n",
    "            new_state_dict[f\"blocks.{layer}.attn.b_O\"] = old_state_dict[\n",
    "                f\"{layer_key}.attn.c_proj.bias\"\n",
    "            ]\n",
    "\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "device = \"cpu\"\n",
    "MODEL_DIR = \"models/\"\n",
    "n_heads = 8\n",
    "n_layers = 8\n",
    "d_model = 512\n",
    "\n",
    "model_name = f\"lichess_{n_layers}layers_ckpt_no_optimizer.pt\"\n",
    "assert str(n_layers) in model_name\n",
    "\n",
    "if not os.path.exists(f\"{MODEL_DIR}{model_name}\"):\n",
    "    state_dict = utils.download_file_from_hf(\"adamkarvonen/chess_llms\", model_name)\n",
    "    model = torch.load(state_dict, map_location=device)\n",
    "    torch.save(model, f\"{MODEL_DIR}{model_name}\")\n",
    "\n",
    "checkpoint = torch.load(f\"{MODEL_DIR}{model_name}\", map_location=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "_orig_mod.transformer.wte.weight torch.Size([32, 512])\n",
      "_orig_mod.transformer.wpe.weight torch.Size([1023, 512])\n",
      "_orig_mod.transformer.h.0.ln_1.weight torch.Size([512])\n",
      "_orig_mod.transformer.h.0.attn.c_attn.weight torch.Size([1536, 512])\n",
      "_orig_mod.transformer.h.0.attn.c_proj.weight torch.Size([512, 512])\n",
      "_orig_mod.transformer.h.0.ln_2.weight torch.Size([512])\n",
      "_orig_mod.transformer.h.0.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "_orig_mod.transformer.h.0.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "_orig_mod.transformer.h.1.ln_1.weight torch.Size([512])\n",
      "_orig_mod.transformer.h.1.attn.c_attn.weight torch.Size([1536, 512])\n",
      "_orig_mod.transformer.h.1.attn.c_proj.weight torch.Size([512, 512])\n",
      "_orig_mod.transformer.h.1.ln_2.weight torch.Size([512])\n",
      "_orig_mod.transformer.h.1.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "_orig_mod.transformer.h.1.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "_orig_mod.transformer.h.2.ln_1.weight torch.Size([512])\n",
      "_orig_mod.transformer.h.2.attn.c_attn.weight torch.Size([1536, 512])\n",
      "_orig_mod.transformer.h.2.attn.c_proj.weight torch.Size([512, 512])\n",
      "_orig_mod.transformer.h.2.ln_2.weight torch.Size([512])\n",
      "_orig_mod.transformer.h.2.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "_orig_mod.transformer.h.2.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "_orig_mod.transformer.h.3.ln_1.weight torch.Size([512])\n",
      "_orig_mod.transformer.h.3.attn.c_attn.weight torch.Size([1536, 512])\n",
      "_orig_mod.transformer.h.3.attn.c_proj.weight torch.Size([512, 512])\n",
      "_orig_mod.transformer.h.3.ln_2.weight torch.Size([512])\n",
      "_orig_mod.transformer.h.3.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "_orig_mod.transformer.h.3.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "_orig_mod.transformer.h.4.ln_1.weight torch.Size([512])\n",
      "_orig_mod.transformer.h.4.attn.c_attn.weight torch.Size([1536, 512])\n",
      "_orig_mod.transformer.h.4.attn.c_proj.weight torch.Size([512, 512])\n",
      "_orig_mod.transformer.h.4.ln_2.weight torch.Size([512])\n",
      "_orig_mod.transformer.h.4.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "_orig_mod.transformer.h.4.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "_orig_mod.transformer.h.5.ln_1.weight torch.Size([512])\n",
      "_orig_mod.transformer.h.5.attn.c_attn.weight torch.Size([1536, 512])\n",
      "_orig_mod.transformer.h.5.attn.c_proj.weight torch.Size([512, 512])\n",
      "_orig_mod.transformer.h.5.ln_2.weight torch.Size([512])\n",
      "_orig_mod.transformer.h.5.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "_orig_mod.transformer.h.5.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "_orig_mod.transformer.h.6.ln_1.weight torch.Size([512])\n",
      "_orig_mod.transformer.h.6.attn.c_attn.weight torch.Size([1536, 512])\n",
      "_orig_mod.transformer.h.6.attn.c_proj.weight torch.Size([512, 512])\n",
      "_orig_mod.transformer.h.6.ln_2.weight torch.Size([512])\n",
      "_orig_mod.transformer.h.6.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "_orig_mod.transformer.h.6.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "_orig_mod.transformer.h.7.ln_1.weight torch.Size([512])\n",
      "_orig_mod.transformer.h.7.attn.c_attn.weight torch.Size([1536, 512])\n",
      "_orig_mod.transformer.h.7.attn.c_proj.weight torch.Size([512, 512])\n",
      "_orig_mod.transformer.h.7.ln_2.weight torch.Size([512])\n",
      "_orig_mod.transformer.h.7.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "_orig_mod.transformer.h.7.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "_orig_mod.transformer.ln_f.weight torch.Size([512])\n",
      "_orig_mod.lm_head.weight torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "# Print the keys of the checkpoint dictionary\n",
    "model_state = checkpoint[\"model\"]\n",
    "print(len(model_state))\n",
    "for key, value in model_state.items():\n",
    "    print(key, value.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_orig_mod.transformer.wte.weight torch.Size([32, 512])\n",
      "_orig_mod.transformer.wpe.weight torch.Size([1023, 512])\n",
      "_orig_mod.transformer.h.0.ln_1.weight torch.Size([512])\n",
      "_orig_mod.transformer.h.0.attn.c_attn.weight torch.Size([1536, 512])\n",
      "_orig_mod.transformer.h.0.attn.c_proj.weight torch.Size([512, 512])\n",
      "_orig_mod.transformer.h.0.ln_2.weight torch.Size([512])\n",
      "_orig_mod.transformer.h.0.mlp.c_fc.weight torch.Size([2048, 512])\n",
      "_orig_mod.transformer.h.0.mlp.c_proj.weight torch.Size([512, 2048])\n",
      "_orig_mod.transformer.ln_f.weight torch.Size([512])\n",
      "_orig_mod.lm_head.weight torch.Size([32, 512])\n",
      "Moving model to device:  cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "LOAD_AND_CONVERT_CHECKPOINT = True\n",
    "\n",
    "if LOAD_AND_CONVERT_CHECKPOINT:\n",
    "    synthetic_checkpoint = model_state\n",
    "    for name, param in synthetic_checkpoint.items():\n",
    "        if name.startswith(\"_orig_mod.transformer.h.0\") or not name.startswith(\n",
    "            \"_orig_mod.transformer.h\"\n",
    "        ):\n",
    "            print(name, param.shape)\n",
    "\n",
    "    cfg = HookedTransformerConfig(\n",
    "        n_layers=n_layers,\n",
    "        d_model=d_model,\n",
    "        d_head=int(d_model / n_heads),\n",
    "        n_heads=n_heads,\n",
    "        d_mlp=d_model * 4,\n",
    "        d_vocab=32,\n",
    "        n_ctx=1023,\n",
    "        act_fn=\"gelu\",\n",
    "        normalization_type=\"LNPre\",\n",
    "    )\n",
    "    model = HookedTransformer(cfg)\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HookedTransformer(\n",
      "  (embed): Embed()\n",
      "  (hook_embed): HookPoint()\n",
      "  (pos_embed): PosEmbed()\n",
      "  (hook_pos_embed): HookPoint()\n",
      "  (blocks): ModuleList(\n",
      "    (0-7): 8 x TransformerBlock(\n",
      "      (ln1): LayerNormPre(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (ln2): LayerNormPre(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (hook_k): HookPoint()\n",
      "        (hook_q): HookPoint()\n",
      "        (hook_v): HookPoint()\n",
      "        (hook_z): HookPoint()\n",
      "        (hook_attn_scores): HookPoint()\n",
      "        (hook_pattern): HookPoint()\n",
      "        (hook_result): HookPoint()\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (hook_pre): HookPoint()\n",
      "        (hook_post): HookPoint()\n",
      "      )\n",
      "      (hook_attn_in): HookPoint()\n",
      "      (hook_q_input): HookPoint()\n",
      "      (hook_k_input): HookPoint()\n",
      "      (hook_v_input): HookPoint()\n",
      "      (hook_mlp_in): HookPoint()\n",
      "      (hook_attn_out): HookPoint()\n",
      "      (hook_mlp_out): HookPoint()\n",
      "      (hook_resid_pre): HookPoint()\n",
      "      (hook_resid_mid): HookPoint()\n",
      "      (hook_resid_post): HookPoint()\n",
      "    )\n",
      "  )\n",
      "  (ln_final): LayerNormPre(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (unembed): Unembed()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_and_process_state_dict(convert_nanogpt_weights(synthetic_checkpoint, cfg))\n",
    "recorded_model_name = model_name.split(\".\")[0]\n",
    "torch.save(model.state_dict(), f\"{MODEL_DIR}tf_lens_{recorded_model_name}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HookedTransformer(\n",
      "  (embed): Embed()\n",
      "  (hook_embed): HookPoint()\n",
      "  (pos_embed): PosEmbed()\n",
      "  (hook_pos_embed): HookPoint()\n",
      "  (blocks): ModuleList(\n",
      "    (0-7): 8 x TransformerBlock(\n",
      "      (ln1): LayerNormPre(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (ln2): LayerNormPre(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (hook_k): HookPoint()\n",
      "        (hook_q): HookPoint()\n",
      "        (hook_v): HookPoint()\n",
      "        (hook_z): HookPoint()\n",
      "        (hook_attn_scores): HookPoint()\n",
      "        (hook_pattern): HookPoint()\n",
      "        (hook_result): HookPoint()\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (hook_pre): HookPoint()\n",
      "        (hook_post): HookPoint()\n",
      "      )\n",
      "      (hook_attn_in): HookPoint()\n",
      "      (hook_q_input): HookPoint()\n",
      "      (hook_k_input): HookPoint()\n",
      "      (hook_v_input): HookPoint()\n",
      "      (hook_mlp_in): HookPoint()\n",
      "      (hook_attn_out): HookPoint()\n",
      "      (hook_mlp_out): HookPoint()\n",
      "      (hook_resid_pre): HookPoint()\n",
      "      (hook_resid_mid): HookPoint()\n",
      "      (hook_resid_post): HookPoint()\n",
      "    )\n",
      "  )\n",
      "  (ln_final): LayerNormPre(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (unembed): Unembed()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 32])\n",
      "tensor([[ 6,  4, 27,  9,  0, 27, 10,  0,  7,  4, 19, 28]])\n",
      "tensor([[True, True, True, True, True, True, True, True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "# An example input\n",
    "sample_input = torch.tensor([[15, 6, 4, 27, 9, 0, 25, 10, 0, 7, 4, 19]]).to(device)\n",
    "# sample_input = torch.tensor([[15, 6, 4, 27, 9]])\n",
    "# The argmax of the output (ie the most likely next move from each position)\n",
    "sample_output = torch.tensor([[6, 4, 27, 9, 0, 27, 10, 0, 7, 4, 19, 28]])\n",
    "output1 = model(sample_input)\n",
    "print(output1.size())\n",
    "model_output = output1.argmax(dim=-1)\n",
    "\n",
    "print(model_output)\n",
    "print(sample_output == model_output)\n",
    "\n",
    "# For this particular sample_input, any model with decent chess skill should output sample_output.\n",
    "# So, this assert will definitely fail for a randomly initialized model, and may fail for models with low skill.\n",
    "# But, I've never seen that happen, so I'm keeping it simple for now. For a more robust test, use the nanogpt_to_transformer_lens.ipynb notebook.\n",
    "# This notebook actually runs the sample input through the original nanogpt model, and then through the converted transformer lens model.\n",
    "assert torch.all(sample_output == model_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
